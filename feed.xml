<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.ksidorov.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://www.ksidorov.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-15T19:41:15+00:00</updated><id>https://www.ksidorov.com/feed.xml</id><title type="html">Konstantin Sidorov</title><subtitle>A website where I share my research and sometimes produce hot takes on the optimization field. </subtitle><entry><title type="html">A new approach to solving hard scheduling problems via disjointness</title><link href="https://www.ksidorov.com/blog/2025/unite-and-lead/" rel="alternate" type="text/html" title="A new approach to solving hard scheduling problems via disjointness"/><published>2025-06-15T00:00:00+00:00</published><updated>2025-06-15T00:00:00+00:00</updated><id>https://www.ksidorov.com/blog/2025/unite-and-lead</id><content type="html" xml:base="https://www.ksidorov.com/blog/2025/unite-and-lead/"><![CDATA[<p>I’m excited to share that my latest paper, “Unite and Lead: Finding Disjunctive Cliques for Scheduling Problems,” co-authored with <a href="https://imkomarijnissen.com">Imko Marijnissen</a> and <a href="https://emirdemirovic.com">Emir Demirović</a>, has been accepted to CP 2025! For those who want all the technical details, you can read the full paper <a href="/assets/pdf/cp2025-unite-and-lead.pdf">here</a>. But for everyone else, I wanted to write this post to explain the core ideas in a more accessible way.</p> <h2 id="the-blind-spots-of-specialization">The blind spots of specialization</h2> <p>Imagine you’re managing a very complex project—like building a factory or designing a CPU. You have hundreds of tasks, and each task requires specific resources: a certain number of workers, a particular machine, a specific tool, and so on. Your goal is to create a schedule that completes the project as quickly as possible without any resource conflicts (like needing 11 workers when you only have 10).</p> <p>This is a classic <strong>scheduling problem</strong>. In the world of computer science, we frequently use general-purpose software tools called “solvers” to tackle them. These solvers are smart, but the constraint programming solvers—the ones commonly used in scheduling—have a particular way of looking at the problem: they focus on one constraint at a time.</p> <p>A solver might look at the “workers” constraint and ensure you never schedule too many tasks at once for your team. Then, it will separately look at the “machine” constraint to make sure the machine isn’t double-booked. This is a very effective strategy, but it can create blind spots. By looking at each resource in isolation, the solver can miss the bigger picture—the <em>global structure</em> of the problem that arises from how different constraints interact.</p> <p>In a way, it’s like trying to solve a Sudoku puzzle by only ever looking at one row at a time, then one column at a time, and then one 3x3 box at a time. Sure, this is a plausible strategy, and with proper accounting, it eventually succeeds; however, this way you will miss crucial insights!</p> <h2 id="a-puzzle-that-fools-modern-solvers">A puzzle that fools modern solvers</h2> <p>To see how the local view can fail, consider a small puzzle; in our work, we call it the <em>3n problem</em>. Let’s say you have six tasks. Each requires a different mix of three resources. Your job is to schedule them. Try it for yourself below! You can drag the tasks onto the timeline. <strong>Can you schedule any two tasks at the same time?</strong></p> <iframe class="iframe-resize" src="/assets/html/cp2025-3n.html" frameborder="0" scrolling="no"></iframe> <p>As you surely discovered by this point, you can’t. Any pair of tasks you choose will overload one of the resources. For example:</p> <ul> <li>Two yellow tasks conflict on Resource 1.</li> <li>A yellow and a green task conflict on Resource 1.</li> <li>A yellow and a blue task conflict on Resource 2.</li> </ul> <p>…and so on.</p> <p>Every single pair of tasks is <strong>disjoint</strong>—they cannot overlap in time. The only solution is to schedule all six tasks sequentially, one after the other. This seems obvious when you look at the whole picture. But for a standard constraint solver, it’s surprisingly tricky:</p> <ul> <li>The solver checks the “Resource 1” constraint and sees <em>some</em> pairs of tasks in conflict.</li> <li>Then it checks the “Resource 2” and finds <em>some other</em> conflicting pairs.</li> </ul> <p>But no single resource constraint tells the solver that <em>all</em> tasks are pairwise disjoint. To figure that out, it needs to combine the information from all three resource constraints. Without a mechanism to do this, the solver resorts to brute-force trial and error, which takes exorbitant amounts of time for larger versions of this puzzle.</p> <h2 id="our-solution-the-disjointness-detective">Our solution: the disjointness detective</h2> <p>This is where our work comes in. We give the solver a new set of tools to reason about the global structure of the problem by focusing on this idea of disjointness. Our approach has two main parts.</p> <p><strong>Mining for disjointness</strong>. First, we upgrade the existing parts of the solver to become “disjointness miners.” Their job is to report back whenever they can prove that two tasks cannot overlap. This can be for various reasons:</p> <ul> <li>Resource clash: like in the puzzle above, two tasks need more of a resource than is available.</li> <li>Precedence: task A must finish before task B can start.</li> <li>No-good learning: a solver has learned from a previous search round that scheduling tasks C and D together leads to a dead end.</li> </ul> <p><strong>A <code class="language-plaintext highlighter-rouge">SelectiveDisjunctive</code> propagator</strong>. Next, we introduce a new <code class="language-plaintext highlighter-rouge">SelectiveDisjunctive</code> constraint and a propagator for it. While this does not change the set of feasible schedules, you can think of it as a master detective. It takes all the disjointness clues found by the miners and puts them together on a big <em>conflict graph</em>. Each task is a node on the graph, and an edge is drawn between any two tasks that are marked as disjoint by miners.</p> <p>The detective’s job is to look for cliques in this graph: groups of tasks where every task is connected to every other task in the group. In our context, any clique is a set of tasks that must all be scheduled sequentially. Once a clique is found, the detective does a quick calculation: it sums up the durations of all tasks in the clique. If that total duration is longer than the available time window for those tasks, it’s an “Aha!” moment. The detective has found a conflict, and it tells the solver, “This branch of the search is impossible. Backtrack now!”</p> <p>Here’s an interactive visualization of that process. Each node corresponds to a task that takes a single time unit; the numbers in the node give a time interval available for a task: 1..6 means that a task starts at time 1 or later and finishes at time 6 or earlier. Click the toggles to apply different resource constraints. As you do, you’ll see the “disjointness” edges appear. Watch what happens when a clique forms!</p> <iframe class="iframe-resize" src="/assets/html/cp2025-clique.html" frameborder="0" scrolling="no"></iframe> <p>This ability to find conflicting cliques by combining information from many different constraints is the superpower of our new approach. It gives the solver a global perspective that it was previously lacking.</p> <h2 id="and-does-it-work">And does it work?</h2> <p>In a word: <strong>yes</strong>!</p> <p>When we tested our approach on the 3n puzzle that stumped other solvers, ours solved it instantly, and for much larger versions at that. More importantly, when we applied it to well-known, difficult scheduling benchmarks from the research community, we saw some major improvements. On problems with high <em>resource contention</em>—meaning resources are scarce and tasks are constantly competing—our approach sometimes improved the solver’s speed by <strong>orders of magnitude</strong>. (That means instead of taking hours, it took minutes or even seconds!)</p> <p>We were also thrilled to discover <strong>new state-of-the-art bounds</strong> for a handful of problems that have been studied by researchers for years. We managed to:</p> <ul> <li>discover the best-known solutions for two RCPSP/max and four RCPSP instances,</li> <li>improve the lower bounds on the achievable objective value for sixteen RCPSP/max and four RCPSP instances,</li> <li>and completely solve seven instances that were previously open along the way. For us, it’s like finding a new, faster route through a well-trodden maze.</li> </ul> <h2 id="the-takeaways">The takeaways</h2> <p>The big lesson from our work is that for complex problems, looking at the pieces in isolation isn’t enough. By developing ways for the solver to unite information from many sources and reason about the global structure of the problem, we can achieve breakthroughs that were previously out of reach.</p> <p>There’s still a lot more to explore, but we’re excited about this new direction for building smarter and more powerful constraint solvers. Thanks for reading!</p>]]></content><author><name></name></author><category term="paper-announcement"/><category term="cp"/><category term="scheduling"/><summary type="html"><![CDATA[An overview of the scheduling technique explored by us in our CP 2025 conference submission.]]></summary></entry><entry><title type="html">CP 2024 impressions</title><link href="https://www.ksidorov.com/blog/2024/cp2024/" rel="alternate" type="text/html" title="CP 2024 impressions"/><published>2024-09-06T00:00:00+00:00</published><updated>2024-09-06T00:00:00+00:00</updated><id>https://www.ksidorov.com/blog/2024/cp2024</id><content type="html" xml:base="https://www.ksidorov.com/blog/2024/cp2024/"><![CDATA[<p>In this post, I want to share some of my takeaways from the <a href="https://cp2024.a4cp.org/">CP 2024</a> conference this week in Girona. While this event, as usual, was stacked with a variety of high-quality optimization talks, I can see a few overarching themes I think would be worthwhile to share.</p> <p>Of course, condensing a week’s worth of technical content to a single post can never have a hint of completeness in it, therefore, I will do my best to share a general impression of the field as I see it without trying to dive into any specific technical details. If you are in Delft or nearby, however, I would welcome you to <a href="https://m-o-o.org/sessions/2024/09/09/">the MOO session</a> this Monday, September 9, where I will lead a more in-depth discussion over the same narrative points.</p> <p>Now, without further ado, let’s dive into the first point I want to make, which is…</p> <h2 id="good-old-fashioned-ai-is-here-to-stay"><a href="https://en.wikipedia.org/wiki/GOFAI">Good old-fashioned AI</a> is here to stay</h2> <p>One of the major takeaways from this year’s papers is that <strong>some of the less known or recognized ideas from earlier days are worth being re-explored</strong>, at least because the computational resources we have now do not look like anything we have had in the heyday of <a href="#" data-toggle="tooltip" data-original-title="Good old-fashioned AI">GOFAI</a>. As for the specific examples, I cannot avoid mentioning an invited talk by Ian Gent <a class="citation" href="#gent:LIPIcs.CP.2024.1">(Gent 2024)</a> on <em>Solvitaire</em>, software for evaluating the winnability of solitaire games which narrowed the winnability range of Klondike to a fraction of a percentage point <a class="citation" href="#blake2024winnabilityklondikesolitairepatience">(Blake and Gent 2024)</a>. This approach achieves impressive computational results – but it does so without reaching to ideas unrelated to search: a good way to summarize this approach would be “running a depth-first search, maintaining the transposition table, and using the dominance pruning rules.”</p> <p>Another direction worth mentioning here is <a href="https://didp.ai/"><em>domain-independent dynamic programming</em></a>, introduced with a tutorial by Chris Beck. Dynamic programming is one of the foundational CS curriculum topics, yet most of the attention in this context is diverted to ad-hoc implementation of various techniques. An approach that brings it closer to model-and-solve is certainly welcome!</p> <p>For a more modern spin, I would like to mention an approach for improving the bin packing propagation with GPUs <a class="citation" href="#tardivo_et_al:LIPIcs.CP.2024.28">(Tardivo, Michel, and Pontelli 2024)</a>. While the first association with GPU programming is surely the linear algebra computations, it turns out you can also use it to parallelize the calculation of different lower bounds, making it useful for getting many uncorrelated lower bounds for the price of one. I think using GPUs in search is yet another underexplored direction; looking forward to more!</p> <p>A hallmark of GOFAI approaches, however, is that they require substantial effort during the implementation to get it right, ranging from establishing the workflow to correcting the smallest of implementation details, or, as Ian Gent has put it, it takes:</p> <blockquote><p>Punctilious tenacious precision.</p><cite><a class="citation" href="#gent:LIPIcs.CP.2024.1">(Gent 2024)</a></cite></blockquote> <p>Given that, it is hardly surprising that more than a few papers support the next major trend:</p> <h2 id="trust-is-an-appreciating-asset">Trust is an appreciating asset</h2> <p><strong>Disclaimer</strong>: both of the papers I have co-authored for this conference fall into this group, so I have an inherent bias towards this subfield.</p> <p>To counter-balance the previous point, several papers have explored ways to <em>justify</em> various claims made by optimization algorithms. To start, a paper by Berg et al. has introduced an approach for justifying a <em>without loss of generality</em> reasoning in <em>Pacose</em> MaxSAT solver <a class="citation" href="#berg_et_al:LIPIcs.CP.2024.4">(Berg et al. 2024; Paxian, Reimer, and Becker 2018)</a>. The key challenge behind justifying this type of reasoning is that it introduces statements that are not implied but do not <em>really</em> change the problem: think symmetry breaking or pure clause elimination.</p> <p>Justifying a claim by any solver is tricky, but the situation for the <em>constraint</em> solvers is particularly hard because they employ a wide range of reasoning techniques to propagate through different constraints. We addressed this in <a class="citation" href="#flippo_et_al:LIPIcs.CP.2024.11">(Flippo et al. 2024)</a> with an approach that first writes the proof <em>scaffold</em> and expands the propagations necessary to justify the “scaffolded” claims later on.</p> <p>All of this involves justifying different parts of model-and-solve workflows; not every optimization algorithm can be described this way, however, with dynamic programming being a notable exception. We proposed a way to do this in <a class="citation" href="#demirovic_et_al:LIPIcs.CP.2024.9">(Demirović et al. 2024)</a> by encoding the dynamic programming states with new variables and mirroring the individual transitions.</p> <p>All in all, as the search algorithms become more important, the need for justifying their results naturally grows as well. Luckily, this is much less of a problem than in, say, learning-based approaches. If you have an idea of how to justify a decision-making procedure (and what to justify in the first place), there is a good chance it has either not been executed yet or differs in some important context; I think we will see more of this action in the next CP conference!</p> <h2 id="incomplete-search-is-worth-more-than-a-passing-mention">Incomplete search is worth more than a passing mention</h2> <p>In many problems, however, the optimality claim is not an asset; the only thing that matters is finding good solutions. Quite a few papers explored this idea this year; in fact, the best paper award went to the work that proposed new local search operators for MIP solving, showing impressive results on the MIPLIB <a class="citation" href="#lin_et_al:LIPIcs.CP.2024.19">(Lin, Zou, and Cai 2024)</a>. Given that many MIP models stem from practical settings without a meaningful interpretation for a lower bound, this certainly looks like a promising result!</p> <p>Another noteworthy work by Chen et al. introduces an approach for running a local search for pseudo-Boolean optimization problems <em>in parallel</em> <a class="citation" href="#chen_et_al:LIPIcs.CP.2024.5">(Chen et al. 2024)</a>. The parallelism here serves both as a way to run many parallel searches and as a remedy against local optima which shares good solutions to re-fire searches that will stall later.</p> <p>I have to admit I do not have a clear idea of what is missing in this subfield of optimization. On the other hand, the local search seems to be an evergreen idea that is always on the table… and clearly useful to keep in mind.</p> <h2 id="data-generated-by-search-algorithms-is-underexploited">Data generated by search algorithms is underexploited</h2> <p>Most of the talks mentioned above were about various techniques for or around the search algorithms; however, there is undeniably a lot of value in learning from data. On a surface, this area does seem to be fit for learning approaches, as the algorithms have to make a lot of decisions and therefore produce a lot of data. However, my takeaway from this year is that <strong>there are many more patterns to mine</strong> in data produced by optimization algorithms than we do now.</p> <p>That is not to say, of course, that there is no effort; quite the contrary, there have been a few exciting talks showing new ways of working with data “in the loop”. For example, <a class="citation" href="#parjadis_et_al:LIPIcs.CP.2024.22">(Parjadis et al. 2024)</a> suggest training a GNN model to produce good Lagrangean multiplies for the traveling salesperson problem. I think this is a promising approach, not least because it seems to only assume that the problem in hand has a “convenient” Lagrangian relaxation; I wonder if we see this approach used in more problem domains next year.</p> <p>There is also a lot to say about reasoning over the <em>constraints</em> of a problem. <a class="citation" href="#michailidis_et_al:LIPIcs.CP.2024.20">(Michailidis, Tsouros, and Guns 2024)</a> explore this idea with the help of large language models. This seems to be a promising way to address the “open world” problem that appears in applied modeling when the optimization problem typically undergoes many iterations which include both the optimization people and the domain experts. Overall, this seems to be a promising way to make optimization technologies more accessible, hopefully with more applications down the line.  </p> <h2 id="conclusion">Conclusion</h2> <p>Overall, that was an exciting event, and an inspiring one at that; not only there is a lot of action going on, but there are several topics that are clearly in high demand. I expect that over the next year, we will see a lot of new ways of justifying various algorithms—or their parts—as well as novel combinations of search techniques across the field. Of course, the usual suspects of novel local search techniques are new propagators are never off the table:) And that’s it for now; goodbye, Girona, and see you in Glasgow next year!</p> <div class="row mt-3">     <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240906_154721-480.webp 480w,/assets/img/20240906_154721-800.webp 800w,/assets/img/20240906_154721-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240906_154721.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>     </div>     <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20240902_205957-480.webp 480w,/assets/img/20240902_205957-800.webp 800w,/assets/img/20240902_205957-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/20240902_205957.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>     </div> </div>]]></content><author><name></name></author><category term="conference-review"/><category term="conference"/><category term="cp"/><summary type="html"><![CDATA[My take on the key trends from the CP 2024 conference.]]></summary></entry></feed>